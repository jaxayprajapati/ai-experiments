{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58c3f699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ac74222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eb131ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "933d123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gemini client\n",
    "client = genai.Client(api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f92a6583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fileSearchStores/googlefilesearchstore-7bhbfsp1unq4'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a File Search store (optional display name)\n",
    "file_search_store = client.file_search_stores.create(config={'display_name': 'google-file_search-store'})\n",
    "file_search_store.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac1cc06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload and import a file into the File Search store\n",
    "file_name = \"transformer.pdf\"\n",
    "operation = client.file_search_stores.upload_to_file_search_store(\n",
    "    file=file_name,  # Path to your file\n",
    "    file_search_store_name=file_search_store.name,\n",
    "    config={'display_name': 'transformer-file-knowledge-base'}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41b44d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the upload and import operation to complete\n",
    "while not operation.done:\n",
    "    time.sleep(5)\n",
    "    operation = client.operations.get(operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "640acc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of transformers, an attention mechanism is a core component that allows the model to weigh the importance of different parts of the input sequence when processing each element of the sequence. It can be understood as a function that maps a query and a set of key-value pairs to an output. This output is calculated as a weighted sum of the values, where the weight assigned to each value is determined by a compatibility function between the query and its corresponding key.\n",
      "\n",
      "There are several types of attention, but the Transformer model primarily uses \"Scaled Dot-Product Attention\" and \"Multi-Head Attention\".\n",
      "\n",
      "### Scaled Dot-Product Attention\n",
      "\n",
      "Scaled Dot-Product Attention takes as input queries (Q), keys (K), and values (V). All of these are vectors. To compute the attention, the dot products of the query with all keys are calculated. Each of these dot products is then divided by the square root of the dimension of the keys ($\\sqrt{d_k}$) to prevent large values from pushing the softmax function into regions with extremely small gradients. After scaling, a softmax function is applied to these results to obtain the weights for the values.\n",
      "\n",
      "The mathematical formula for Scaled Dot-Product Attention, when queries, keys, and values are packed into matrices Q, K, and V respectively, is:\n",
      "$Attention(Q,K,V)=softmax(\\frac{QK^{T}}{\\sqrt{d_{k}}})V$\n",
      "\n",
      "### Multi-Head Attention\n",
      "\n",
      "Multi-Head Attention enhances the attention mechanism by allowing the model to jointly attend to information from different representation subspaces at various positions. Instead of performing a single attention function, Multi-Head Attention linearly projects the queries, keys, and values multiple times (h times) with different learned linear projections. These projections result in different sets of queries, keys, and values, each with a reduced dimensionality.\n",
      "\n",
      "For each of these projected sets, the attention function (Scaled Dot-Product Attention) is performed in parallel, yielding multiple output values. These outputs, known as \"heads,\" are then concatenated and projected once more to produce the final result. This parallel processing with different projections allows the model to capture diverse aspects of relationships within the data.\n",
      "\n",
      "### Applications of Attention in the Transformer Model\n",
      "\n",
      "The Transformer model utilizes multi-head attention in three distinct ways:\n",
      "\n",
      "1.  **Encoder-Decoder Attention:** In the decoder, these layers receive queries from the previous decoder layer, while the keys and values come from the output of the encoder. This setup enables every position in the decoder to attend over all positions in the input sequence, mimicking typical encoder-decoder attention mechanisms.\n",
      "2.  **Self-Attention in the Encoder:** The encoder contains self-attention layers where the queries, keys, and values all originate from the output of the previous layer within the encoder itself. This allows each position in the encoder to attend to all positions in the preceding layer of the encoder, forming a representation of the sequence by relating different positions within it.\n",
      "3.  **Masked Self-Attention in the Decoder:** Similar to the encoder, the decoder also uses self-attention layers. However, these are \"masked\" to prevent information flow from future positions. This means that each position in the decoder can only attend to earlier positions up to and including its own position, preserving the auto-regressive property required for sequence generation. This masking is implemented by setting illegal connections in the input of the softmax to negative infinity.\n"
     ]
    }
   ],
   "source": [
    "# Query the file store with a question\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is a attention mechanism in transformers?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[\n",
    "            types.Tool(\n",
    "                file_search=types.FileSearch(\n",
    "                    file_search_store_names=[file_search_store.name]\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print the response text\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9ad697d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='fileSearchStores/googlefilesearchstore-pco2z4ua6wmq' display_name='google-file_search-store' create_time=datetime.datetime(2025, 11, 23, 22, 27, 14, 93213, tzinfo=TzInfo(0)) update_time=datetime.datetime(2025, 11, 23, 22, 27, 14, 93213, tzinfo=TzInfo(0)) active_documents_count=1 pending_documents_count=None failed_documents_count=None size_bytes=2215244\n",
      "name='fileSearchStores/transformerfileknowledgebas-ld3y7t66glh5' display_name='transformer-file-knowledge-base' create_time=datetime.datetime(2025, 11, 23, 22, 32, 58, 630900, tzinfo=TzInfo(0)) update_time=datetime.datetime(2025, 11, 23, 22, 32, 58, 630900, tzinfo=TzInfo(0)) active_documents_count=None pending_documents_count=None failed_documents_count=None size_bytes=None\n",
      "name='fileSearchStores/googlefilesearchstore-7bhbfsp1unq4' display_name='google-file_search-store' create_time=datetime.datetime(2025, 11, 23, 22, 34, 35, 890535, tzinfo=TzInfo(0)) update_time=datetime.datetime(2025, 11, 23, 22, 34, 35, 890535, tzinfo=TzInfo(0)) active_documents_count=1 pending_documents_count=None failed_documents_count=None size_bytes=2215244\n",
      "name='fileSearchStores/transformerfileknowledgebas-ckk57s5iudc7' display_name='transformer-file-knowledge-base' create_time=datetime.datetime(2025, 11, 23, 22, 35, 3, 739845, tzinfo=TzInfo(0)) update_time=datetime.datetime(2025, 11, 23, 22, 35, 3, 739845, tzinfo=TzInfo(0)) active_documents_count=None pending_documents_count=None failed_documents_count=None size_bytes=None\n"
     ]
    }
   ],
   "source": [
    "# Create a File Search store (including optional display_name for easier reference)\n",
    "file_search_store = client.file_search_stores.create(config={'display_name': 'transformer-file-knowledge-base'})\n",
    "\n",
    "# List all your File Search stores\n",
    "for file_search_store in client.file_search_stores.list():\n",
    "    print(file_search_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4f7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
